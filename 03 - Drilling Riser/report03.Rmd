---
title: 'Estudo de Caso 03: Comparison of Rising Drilling Configurations'
author: "Equipe 04"
date: "24 de Junho de 2017"
output:
  pdf_document:
    fig_caption: yes
  word_document: default
header-includes:
- \usepackage{fancyhdr}
- \usepackage[utf8]{inputenc}
- \pagestyle{fancy}
- \fancyfoot[CO,CE]{EEE933 - Planejamento e Análise de Experimentos}
- \fancyfoot[LE,RO]{\thepage}
csl: ieee.csl
bibliography: bibliography.bib
---  
Coordenador: Bernardo Marques   
Relator: Danny Tonidandel   
Verificador: Gustavo Vieira   
Monitor: Alessandro Cardoso

```{r setup,include=FALSE, results='hide',warning=FALSE,echo=FALSE}
if (!require(readr, quietly = TRUE)){
  install.packages("readr")
}
if (!require(car, quietly = TRUE)){
  install.packages("car")
}
if (!require(lmtest, quietly = TRUE)){
  install.packages("lmtest")
}
source('calcN.R')
library(car)
library(lmtest)

```

# 1. Descrição do Problema
Objetiva-se realizar uma comparação entre quatro tipos de tubos de perfuração _(drilling risers)_, que consiste em uma espécie de conduíte ou tubo utilizado para servir como passagem temporária para o petróleo extraído em plataformas oceânicas. Mais especificamente, pretende-se comparar o tempo médio até a falha _(mean time to failure ou MTTF)_ de quatro configurações diferentes (níveis $A$, $B$, $C$, $D$) de equipamentos, de forma a escolher a que forneça a menor probabilidade de falha, considerando-se um período de $20$ anos. Para isto será comparada a configuração padrão _(Riser1)_ com as outras três, buscando encontrar qual delas proverá o maior _MTTF_, que pode ser sumarizado na seguinte questão:

"Algum dos risers alternativos é melhor que o padrão?"

A plataforma de testes escolhida pela equipe de engenharia consiste na utilização de um modelo em escala para com um protocolo de tempo acelerado, em que há uma relação direta entre o tempo medido de cada observação (em minutos) e a configuração do sistema real. Todavia, o custo de cada observação é significativamente alto, cerca de $US\$10.000$ (dez mil dólares). A fim de minimizar os custos com amostras coletadas, tem-se, como alternativa, utilizar dados históricos existentes para a primeira configuração _(Riser1)_.

```{r,results='show',warning=FALSE,echo=FALSE}
## EXPERIMENTAL DEFINITIONS
v_alpha = 0.05
v_a = 4
v_k = v_a-1
v_alphaAdj = v_alpha/v_k
v_beta = 0.15
v_power = 1-v_beta
v_delta = 0.25
```
Os parâmetros experimentais desejados são:

* Nível de significância: $\alpha = 0.05$;

* Tamanho de efeito de interesse prático: ${d^{\star}_{t}} = 0.25$;

* Potência desejada: $(1- \beta) = \pi  \geq 0.85$.

# 2. Planejamento Experimental

Esta etapa consiste em investigar o comportamento dos níveis de fator a partir de uma análise exploratória, e, posteriormente, aplicar um teste de comparações múltiplas para as médias. A variância do processo é desconhecida e será estimada utilizando-se os dados históricos de operação fornecidos em <https://git.io/vHDG3>. A variância será considerada como sendo uniforme para todas as configurações.

Os dados experimentais utilizados foram obtidos através de simulação, por meio de um [aplicativo web] <http://orcslab.cpdee.ufmg.br:3838/riserdata/>. A data de nascimento do segundo membro mais jovem da equipe (15/10/1992) foi o parâmetro utilizado como semente para o gerador de números da simulação. Os dados gerados informam uma tabela com os níveis de cada fator de interesse em uma coluna e os tempos (tomados em escala logarítmica) em coluna respectiva. Por este motivo não necessitam de tranformações inversas.

## 2.1 Análise de Variância
 
Para a avaliar o questionamento acima, será utilizada a ferramenta estatística para análise de variância denominada ANOVA.

A técnica ANOVA compara médias de diferentes amostras para verificar se estas possuem médias iguais ou não. Assim, essa técnica permite que vários grupos sejam comparados simultâneamente [@Campelo2015-01], [@MontgomeryRunger2011]. Em outras palavras, a análise de variância é utilizada quando se quer decidir se os níveis apresentam médias diferentes em relação é uma média global $\mu$. As diferenças entre as médias dos níveis e a média global é  $\tau_i$ $\forall \  i$. 

Vale ressaltar que a técnica é restrita apenas à indicação de existência ou não de diferenças entre os níveis avaliados, sem indicar quais níveis seriam diferentes. Além disso, quando a análise de variância tem como resultado um indicativo de refutação da hipótese nula $\eqref{eq:01}$ é que podem ser evidenciados os indícios de diferenças entre os níveis.


\begin{equation}
\left\{\begin{array}{rc}
H_{0}:& \tau_i = 0, \ \ \ \forall i\\
H_{1}:& \exists \ \ \tau_i \neq 0
\end{array}\right.
\label{eq:01}
\end{equation}

Esta etapa foi considerada como uma primeira verificação ao questionamento deste estudo por apresentar um custo de coleta de amostras menor do que o teste de comparações múltiplas discutido abaixo. Portanto, caso não seja detectada alguma diferenças entre os níveis o estudo será concluído por meio do teste ANOVA.

## 2.2 Comparações Múltiplas

Caso a análise ANOVA identifique a existência de diferenças entre os níveis, deve-se proceder com testes de comparação múltiplas todos contra um, no intuito de identificar qual ou quais níveis apresentam tal diferença.

Desta forma, somente, caso a análise de variância indique a existência de diferenças entre os níveis, será aplicado o teste de comparações múltiplas um-contra-todos (one-vs-all) de _Dunnett_, onde os _Risers_ propostos serão confrontados com a configuração padrão _Riser1_ para verificar se alguma proposta traria ganho de _MTTF_ frente à configuração já estabelecida.

Para que se proceda com o teste de comparações múltiplas, deve-se manter o controle sobre os erros do tipo-$I$ em cada comparação. É preciso corrigir os valores de $\alpha$ para cada teste. A escolha para este caso é o método de correção de Bonferroni $\eqref{eq:02}$:
\begin{equation}
\alpha_{adj} = \frac{\alpha_{família}}{K} \,,
\label{eq:02}
\end{equation}
no qual $K=3$ consiste no número de comparações a serem feitas que no caso do teste um-contra-todos de _Dunnett_ ($eq. \eqref{eq:03}$):
\begin{equation}
K=a-1  \,;
\label{eq:03}
\end{equation}
sendo que $a=4$ consiste no número de níveis e $\alpha$ (experimental) desejado $\alpha_{família} = 0.05$. Assim, o valor de $\alpha_{adj} = 0.0167$.

## 2.3 Definição do Tamanho Amostral
Para calcular o tamanho amostral serãos utilizadas as mesmas relações para a comparação de duas amostras independentes emparelhadas ("todos contra um"), alterando-se apenas os valores de $\alpha$ para os valores corrigidos $\alpha_{adj}$, considerando-se as múltiplas hipóteses e $a-1$ comparações. O tamanho amostral para o teste unilateral para as configurações que serão comparadas ao _riser1_ é calculado conforme a equação $\eqref{eq:04}$:
\begin{equation}
n_i = \left(1 + \frac{1}{K}\right) \left( \frac{\hat{\sigma}}{\delta^{\star}}\right)^{2}(t_{\alpha_{adj}}+t_{\beta})^{2} \,,
\label{eq:04}
\end{equation}
em que $t_{\alpha_{adj}}$ e $t_{\beta}$ são dependentes de $n$.

Para solucionar o problema da dependência de $n$, os termos $t_{\alpha_{adj}}$ e $t_{\beta}$ foram substituídos por $z_{\alpha_{adj}}$ e $z_{\beta}$ e a equação foi testada iterativamente até a convergência (implementação em anexo no arquivo _calcN.R_). Dessa forma, foi encontrado um primeiro valor para o tamanho amostral $n_i = 58$.

Contudo, para aumentar a potência do procedimento de múltiplas comparações, o tamanho amostral do grupo de controle deve ser calculado conforme a equação $\eqref{eq:05}$ [@Campelo2015-01]:
\begin{equation}
n_0 = n_i\sqrt{K} \,,
\label{eq:05}
\end{equation}
Lembrando que $K$ é o número de comparações, conforme  atesta a relação $\eqref{eq:03}$, têm-se, portanto $n_i = 101$.

```{r,results='show',warning=FALSE,echo=FALSE}
## SAMPLE SIZE CALC
v_dataHist = readr::read_csv('riser1.csv')
v_sdHist = sd(v_dataHist$LogTTF)

v_n = calcN_oneVsAll(p_alpha = v_alphaAdj,
            p_beta = v_beta,
            p_alternative = 'one-sided',
            p_k = v_k,
            p_sd = v_sdHist,
            p_delta = v_delta
)

v_nControl = ceiling(v_n*sqrt(v_k))

v_tau = c(-v_delta*(v_a-1)/v_a, rep(v_delta/v_a, v_k))
vartau = var(v_tau)
v_nAnova = power.anova.test(groups = v_a, 
                            between.var = vartau, 
                            within.var = v_sdHist^2, 
                            sig.level = v_alpha, 
                            power = v_power)$n

v_nControl = ceiling(v_n*sqrt(v_k))

v_tau = c(-v_delta*(v_a-1)/v_a, rep(v_delta/v_a, v_k))
#v_tau = c(-v_delta/2, v_delta/2, rep(0, v_k-1))
vartau = var(v_tau)
v_nAnova = power.anova.test(groups = v_a, 
                            between.var = vartau, 
                            within.var = v_sdHist^2, 
                            sig.level = v_alpha, 
                            power = v_power)$n

```

O cálculo do tamanho amostral foi igualmente realizado para a técnica ANOVA: isto pode ser feito a partir de sucessivas iterações em $n$ até que
\begin{equation}
F_{(1-\alpha)}=F_{\beta;\phi} \,,
\label{eq:06}
\end{equation}
em que ambas distribuições $F$ têm $(a-1)$ graus de liberdade no numerador e $a(n-1)$ no denomiador. Além disso, o parâmetro de não-centralidade $\phi$ é dado por:
\begin{equation}
\phi=(n\sum_{i=1}^{a}\tau^2_i)/\hat\sigma^2 \,.
\label{eq:07}
\end{equation}

O valor de $\tau$, no caso de comparações "todos contra um" pode ser obtido a partir da relação $\eqref{eq:08}$

\begin{equation}
\tau = \left( -\frac{(a-1)\delta^{\star}}{a}, \frac{\delta^{\star}}{a}, \frac{\delta^{\star}}{a}, \frac{\delta^{\star}}{a} \right) \,,
\label{eq:08}
\end{equation}
que produz um valor de tamanho amostral para a análise de variância $n=60$. Desta forma, tem-se a situação:

* Número de amostras necessárias para o ANOVA: $60 \times 3+50=230$

* Número de amostras necessárias a compração todos contra um: $58*3 + 91 =265$

Consequentemente, observa-se que o custo de aplicação a análise de variância será, para este caso, de $\$2.300.000$ e o das comparações $\$2.650.000$. 

Serão necessárias $60$ coletas para a realização do ANOVA e $58$ para as comparações (excetuando-se o grupo controle), o que resultaria em uma sobreamostragem de $2$ unidades por grupo. Entretanto, caso o ANOVA não forneça como resultado a diferença entre as configurações dos _risers_, seriam economizados cerca de $\$350.000$ fazendo o ANOVA antecipadamente. Tais informações sumarizadas na tabela 1.

\begin{table}[]
\centering
\label{tab:01}
\begin{tabular}{|l|l|l|}
\hline
& Melhor caso & Pior caso   \\ \hline
Com ANOVA & \$2.300.000 & \$2.710.000 \\ \hline
Sem ANOVA & \$2.650.000 & \$2.650.000 \\ \hline
Diferença & \$350.000   & -\$60,000   \\ \hline
\end{tabular}
\caption{Análise de custo das opções}
\end{table}

Será seguido o caminho com o ANOVA por causa da grande possível economia e da não tão significante gasto com a sobre amostragem caso o ANOVA identifique diferença entre os risers.

## 2.4 Tratamento e Validação dos Dados

Considerando o experimento realizado, foi criada uma rotina para validação dos dados obtidos e identificação de erros, onde o tempo de _MTTF_ na escala logarítma deve ser maior que $0$. 

1. LogTTF $> 0$

Caso os valores de uma execução não atendam essas condições, ela seria descartada. No entanto, nenhuma das amostras apresenta problema.


# 3. Análise Estatística

## 3.1 Análise de Variância

```{r,results='hide',warning=FALSE,echo=FALSE, fig.height=4, fig.width=6}
v_data = readr::read_csv('1992-10-15_60_60_60_60.csv')
v_data$Riser = as.factor(v_data$Riser)
boxplot(LogTTF~Riser, 
        data = v_data, 
        xlab = "Riser",
        ylab = "LogTTF", 
        main = "Riser data",
        pch  = 16,
        col  = "gray")
v_model <- aov(LogTTF~Riser, 
             data = v_data)
summary.aov(v_model)
```

Como pode ser observado, o gráfico acima não indica visualmente diferença significativa entre os níveis. Corrobora com isto, o _F-valor = 0.608_ do teste ANOVA indicando pela não rejeição da hipótese de que não há diferença entre os grupos.

Este resultado também indica que não há necessidade de testes de comparação múltiplas que verificariam quais níveis teriam se apresentado diferentes do grupo de controle _Riser1_.

## 3.2 Validação das Premissas

### Normalidade

```{r,results='hide',warning=FALSE,echo=FALSE, fig.height=5}
shapiro.test(v_model$residuals)

qqPlot(v_model$residuals, 
       pch = 16, 
       lwd = 3, 
       cex = 2, 
       las = 1)
```

O _p-valor = 0.299_ encontrado no teste de _Shapiro-Wilk_ indica pela rejeição normalidade das amostras. No entanto, o qq plot mostra que as violações de normalidade são relativamente pequenas. A análise ANOVA é robusto a pequenas variações de normalidade [@Campelo2015-01]. Desta forma considerou-se a premissa de normalidade atendida.

### Homocedasticidade

```{r,results='hide',warning=FALSE,echo=FALSE, fig.height=3.5}
fligner.test(LogTTF~Riser, 
             data = v_data)

plot(x    = v_model$fitted.values,
     y    = v_model$residuals,
     cex  = 2,
     las  = 1,
     pch  = 16,
     xlab = "Fitted values",
     ylab = "Residuals")
grid(NULL,NULL, lwd=2, col = "#44444422")
```

O teste de igualdade de variância dos resíduos de _Fligner-Killeen_ apresentou um _p-valor = 0.1416_ o que também indica pela rejeição da homocedasticidade das amostras.

Contudo, podemos observar no gráfico acima, uma variância relativamente baixa entre os resíduos das amostras. Novamente com base de que a análise ANOVA é robusto a pequenas variações também de homocedasticidade como indicado por  [@Campelo2015-01], considerou-se a premissa atendida.


### Independência

```{r,results='hide',warning=FALSE,echo=FALSE, fig.height=3.5}
v_dwTest = durbinWatsonTest(v_model)
v_dwTestp = v_dwTest$p

plot(x    = seq_along(v_model$residuals),
     y    = v_model$residuals,
     type = "l",
     las  = 1,
     lwd  = 2,
     lty  = 1,
     xlab = "Residual order",
     ylab = "Residual value")
#points(x    = seq_along(v_model$residuals),
#       y    = v_model$residuals,
#       type = "p",
#       cex  = 2,
#       pch  = 5,
#       col  = as.numeric(v_data[, 2]))
grid(NA,NULL, lwd=2, col = "#44444422")
```

O plot dos valores ordenados de diferenças de tempo entre os algoritmos não apresenta nenhum indício de dependência temporal dos valores. O teste de autocorrelação serial Durbin-Wastson apresenta  $p = `r v_dwTestp`$, o que reforça a hipótese de que não há autocorrelação serial entre as amostras.


# 4. Discussão e Conclusões

Os testes realizados levam às seguintes conclusões:

Variância entre grupos é explicada pela variância intra grupo. Não há indício de diferença significativa entre eles.

Recomenda-se manter riser 1. Custo do experimento é significativo $(\$2.300.000)$, mas previniu um custo potencialmente maior de trocar o Riser.

# Referências